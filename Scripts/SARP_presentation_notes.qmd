---
title: "SARP_presentation_notes"
author: "Carrie Hashimoto"
format: html
pdf-engine: xelatex
---

## Describing Evapotranspiration Reduction Following the Creek Fire

# Motivation

- Wildfire and water balance



# The Creek Fire

-   2020-09-04 through 2020-12-24

-   380,000 acres burned \[x\]

## Data: OpenET

- Uses 6 models

  + ALEXI, eeMETRIC, geeSEBAL, PT-JPL, SIMS, SSEBop
  + most are based on the surface energy balance approach
  + using surface energy balance based on satellite measurements of surface temperature and reflectance, combined with weather and land surfaace characteristics
- input data: Landsat for optical and thermal data; also GOES, Sentinel-2, Suomi NPP, Terra, Aqua, and others
  + also uses weather station measurements
  + uses weather datasets derived from these, like gridMET, spatial CIMIS, DAYMET, PRISM, and NLDAS
  
- Particularly, reference ET from the Penman-Monteith equation is used to estimate actual ET between Landsat passovers

## Model: MLR

- Basic no-interaction full model adj. R2 = 0.5141

Call:
lm(formula = lm_rET_formula, data = df)

Residuals:
    Min      1Q  Median      3Q     Max 
-111.92  -11.92    1.40   13.62  115.62 

Coefficients:
                Estimate Std. Error t value Pr(>|t|)    
(Intercept)   -1.098e+04  6.410e+01 -171.26   <2e-16 ***
lat           -1.408e+02  5.672e-01 -248.14   <2e-16 ***
lon           -1.364e+02  7.039e-01 -193.74   <2e-16 ***
prefire_ndvi  -7.529e+01  1.734e-01 -434.09   <2e-16 ***
dndvi          1.202e+02  5.396e-01  222.78   <2e-16 ***
dnbr           3.497e+01  3.091e-01  113.12   <2e-16 ***
temp          -1.142e+00  2.316e-02  -49.33   <2e-16 ***
precip         8.825e-02  2.234e-03   39.51   <2e-16 ***
elev           2.725e-02  8.007e-05  340.32   <2e-16 ***
aspect        -1.917e-02  1.726e-04 -111.04   <2e-16 ***
slope         -2.138e-01  2.179e-03  -98.12   <2e-16 ***
north         -3.141e+00  8.371e-02  -37.52   <2e-16 ***
soil_moisture -5.172e-01  4.267e-03 -121.22   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 20.5 on 1716885 degrees of freedom
Multiple R-squared:  0.5141,	Adjusted R-squared:  0.5141 
F-statistic: 1.514e+05 on 12 and 1716885 DF,  p-value: < 2.2e-16


# Improved MLR model - reduced with interaction


Call:
lm(formula = lm_rET_interaction_reduced_formula, data = df)

Residuals:
     Min       1Q   Median       3Q      Max 
-114.982   -9.806   -0.568    9.722   99.202 

Coefficients:
                     Estimate Std. Error t value Pr(>|t|)    
(Intercept)         1.664e+00  3.992e-02   41.67   <2e-16 ***
dnbr                2.514e+02  5.627e-01  446.77   <2e-16 ***
dnbr:prefire_ndvi  -2.923e+02  8.165e-01 -357.95   <2e-16 ***
dnbr:soil_moisture  3.106e-01  2.098e-03  148.00   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 16.12 on 532816 degrees of freedom
Multiple R-squared:  0.6181,	Adjusted R-squared:  0.6181 
F-statistic: 2.875e+05 on 3 and 532816 DF,  p-value: < 2.2e-16












## Model: GAM

-   Builds on a multiple linear regression, which is just

$$
y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 + x_{i2} + \cdots + \beta_p x_{ip} + \epsilon_i
$$

-   With a GAM, replace the linear functions of $x_i$ with smooth nonlinear functions like natural or smoothing splines

$$
y_i = \beta_0 + f_1(x_{i1}) + f_2(x_{i2}) + \cdots + f_p(x_{ip}) + \epsilon_i
$$

-   Not much difference between using natural and smoothing splines, but natural are a bit easier to fit

For smoothing splines,

$$
RSS = \sum_{i=1}^n (y_i - g(x_i))^2 + \lambda \int g'' (t)^2 dt
$$

where $\lambda \geq 0$ is the tuning parameter and $g$ that minimizes the RSS is a smoothing spline.

- minimally rough: second derivative is zero, so could be a linear function




# GAM: multivariate considerations
https://stats.stackexchange.com/questions/45446/intuition-behind-tensor-product-interactions-in-gams-mgcv-package-in-r

regular ass normal equation

$$
\beta = (X' X)^{-1} X' \mathbf{y}
$$
smoothness penalty

$$
\beta = (X' X + \lambda S)^{-1} X' \mathbf{y}
$$
- where S is a quadratic i by i penalty matrix

- creates issues when x_i are not all on the same scale

## Tensor product smooths

- address inputs w different units

- don't have to have multivariate marginal bases




















- I should probably start with a MLR model just to see what the general relationships look like

- Backfitting is needed to fit smoothing splines since you can't use LS with them

Possible components to use:

- MLR is useful but limited because sometimes linear approximations are bad

Alternatives to MLR:
These all simplify the linear model and reduce variance, but it's still a linear model
- Ridge regression
- Lasso
- PCA

Non-linear extensions of regression

- Polynomial

- Step function - cut range into K regions

- Regression splines - more flexible than polynomials and step functions, extending both of them; polynomials are fit to each region, but constrained so that they join smoothly at region boundaries

- Smoothing splines - similar to regression splines, but minimize residual SS while maintaining smoothness penalty

- Local regression - similar to splines

- GAMs combine these methods for multiple predictors

## Model Selection
https://rdrr.io/cran/mgcv/man/gam.selection.html
- mgcv package used

- Prediction error for gam() uses *Generalized Approximate Cross Validation*; scale parameter is either unknown or an *Un-Biased Risk Estimator*

- A UBRE is scaled AIC for a generalized case or Cp for the additive model case



**review any Chi material**

- Lecture 14

### Automatic term selection

- Most smoothing penalties consider some functions to be "completely smooth" and start ignorning terms once they're penalized enough to be in this space

- Can change selection so that once penalized to a sufficient extent, they are removed:

-- add another shrinkage term; use smooth classes cs.smooth and tprs.smooth that contain a shrinkage component so that automatic smoothing parameter selection "effectively" removes the terms

-- another alternative: keep the original smoothing penalty, but add another one for every function that only penalizes functions that are inside the null space of the original penalty

## Interactive term selection

- Compare GCV/UBRE/ML scores for models with and without a term in order to decide whether to include it

-- achieve same result with UBRE as you would with AIC

- **restricted maximum likelihood** what is this? cannot use it to compare models with different fixed effects structures

- When would the scale parameter be known? If there were some kind of theoretical background?

### GAM background
https://hastie.su.domains/Papers/ESLII.pdf

- recall logistic GAMs: 
$$
\log \left( \frac{\mu (X)}{1- \mu (X)} \right) = \alpha + f_1 (X_1) + \cdots + f_p (X_p)
$$

Important: the LHS can also be a function:

$$
g[\mu (X)] = \alpha + f_1 (X_1) + \cdots + f_p (X_p)
$$

- Ex could use $g(\mu) = \log (\mu)$

*Aside: parametric vs nonparametric regression*

- Note that functions in GAMs can use attributes other than main effects; ex interactions: functions of multiple variables or different functions of Xj for different levels of a factor Xk

- Can use penalized sum of squares to fit a model - sum of squares with weighted sum of square of second derivative integrated over domain

# Lecture 11 - review K-fold cross validation

file:///Users/Carrie/Desktop/creekfire/short-gam/slides/smooth-toolbox.pdf
- Since predictors aren't all in the same units, should use scale invariant smooths

