---
title: "HLS Aquisition and Processing"
output:
  pdf_document:
    toc: yes
    toc_depth: '2'
theme: lumen
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(comment = NA)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
```

Adapted from HLS R tutorial: https://git.earthdata.nasa.gov/scm/lpdur/hls_tutorial_r.git

Call .netrc file

```{r, warning = FALSE, message = FALSE, results= "hide"}
source("Scripts/earthdata_netrc_setup.R")
```

Check required packages and install as needed.

```{r, warning = FALSE, message = FALSE}
packages <- c('terra','jsonlite','sp','httr',
              'rasterVis','ggplot2','magrittr','RColorBrewer','xml2','dygraphs',
              'xts','lubridate','DT','rmarkdown', 'rprojroot','imager','htmltools',
              'tidyterra','leaflet')

# Identify missing (not installed) packages
new.packages = packages[!(packages %in% installed.packages()[,"Package"])]

# Install new (not installed) packages
if(length(new.packages)) install.packages(new.packages, repos='http://cran.rstudio.com/') else print('All required packages are installed.')
```

Load packages.

```{r, warning= FALSE, message=FALSE}
invisible(lapply(packages, library, character.only = TRUE))
```

Create output directory if needed.

```{r}
# Create an output directory if it doesn't exist
wd <- rprojroot::find_rstudio_root_file()
outDir <- file.path(wd, "Data", "HLS_data", fsep="/")
suppressWarnings(dir.create(outDir)) 
```

Create search query function.

```{r}
get_search <- function(date_range,boundary_str) {
  # input date_range (vec of str) and boundary_str (str)
  search_body <- list(limit=250,
                 datetime=date_range,
                 bbox= boundary_str,
                 collections= HLS_col)
  
  search_req <- httr::POST(search_URL, body = search_body, encode = "json") %>%
    httr::content(as = "text") %>%
    fromJSON()
  
  cat('There are',search_req$numberMatched, 'features that matched our request for', date_ranges[i], "\n")

  return(search_req)
}
```

Set gdal configurations (might not need to do this since rgdal is defunct?).

```{r, results= "hide"}
setGDALconfig("GDAL_HTTP_UNSAFESSL", value = "YES")
setGDALconfig("GDAL_HTTP_COOKIEFILE", value = ".rcookies")
setGDALconfig("GDAL_HTTP_COOKIEJAR", value = ".rcookies")
setGDALconfig("GDAL_DISABLE_READDIR_ON_OPEN", value = "EMPTY_DIR")
setGDALconfig("CPL_VSIL_CURL_ALLOWED_EXTENSIONS", value = "TIF")
```

Bands of interest:

-   Sentinel-2:

    -   "narrow" NIR = B8A
    -   Red = B04
    -   SWIR 1 = B11
    -   SWIR 2 = B12
    -   Quality = Fmask

-   Landsat 8:

    -   NIR = B05
    -   Red = B04
    -   SWIR 1 = B06
    -   SWIR 2 = B07
    -   Quality = Fmask

Create a function to make a searchable datatable from which to retrieve asset urls.
- I don't have a great understanding of this function that came from the tutorial

```{r}
make_table <- function(search_features, i) {

  granule_list <- list()
  n <- 1
  for (item in row.names(search_features)){                       # Get the NIR, Red, SWIR2 and Quality band layer names
    if (search_features[item,]$collection == 'HLSS30.v2.0'){
      ndvi_bands <- c('B8A','B04','B12','Fmask')
    }
    else{
      ndvi_bands <- c('B05','B04','B07','Fmask')
    }
    for(b in ndvi_bands){
      f <- search_features[item,]
      b_assets <- f$assets[[b]]$href
      
      df <- data.frame(Collection = f$collection,                    # Make a data frame including links and other info
                       Granule_ID = f$id,
                       Cloud_Cover = f$properties$`eo:cloud_cover`,
                       band = b,
                       Asset_Link = b_assets, stringsAsFactors=FALSE)
      granule_list[[n]] <- df
      n <- n + 1
    }
  }
  
  # Create a searchable datatable
  search_df <- do.call(rbind, granule_list)
  return(search_df)
}
```

Check .netrc location.

```{r, echo = FALSE, results='hide'}
cat('The netrc file can be found in:', Sys.getenv('HOME'))
```

Create a function to switch the boundary's crs to the crs found in the data.

```{r, warning=FALSE}
update_crs <- function(search_df,boundary_vect) {
  # extract data's crs
  coordinate_reference <- terra::rast(paste0(search_df$Asset_Link[1]))
  
  # update boundary's crs
  boundary_updated <- terra::project(boundary_vect, crs(coordinate_reference)) # Transfer CRS
  # crs(creekfire_geojson_utm)
  return(boundary_updated)
}
```

Create a function to load the rasters from the data table retrieved.

```{r, warning=FALSE, results= "hide"}

load_rasters <- function(search_df, boundary_load_rasters) {
  
  # initialize stacks
  red_stack <- nir_stack <- fmask_stack <- swir2_stack <- list()
  date_vect <- c()
  
  # create progress bar
  pb <- txtProgressBar(min = 0, max = length(search_df$band), initial = 0, style = 3)
  
  # initialize counters for each stack
  l <- m <- n <- k <- 0
  
  # create function to extract date from raster source
  extract_date <- function(granule_id) {
    doy_time <- strsplit(granule_id, "[.]")[[1]][4]
    doy <- substr(doy_time, 1, 7)
    dat <- as.Date(as.integer(substr(doy, 5, 7)), origin = paste0(substr(doy, 1, 4), "-01-01"))
    return(dat)
  }
  
  # go through every row in the search_df
  for (b in 1:length(search_df$band)) {
    
    # increment the progress bar
    setTxtProgressBar(pb, b)
    
    # retrieve this file
    b_layer <- tryCatch({
      terra::rast(paste0('/vsicurl/', search_df$Asset_Link[b]))
    }, error = function(e) {
      warning(paste("Error loading raster:", e))
      return(NULL)
    })
    
    if (is.null(b_layer)) next
    
    lab <- substr(search_df$Granule_ID[b], 5, 5)
    dat <- extract_date(search_df$Granule_ID[b])
    
    # project it to WGS
    b_layer <- tryCatch({
      terra::project(b_layer, "EPSG:4326")
    }, error = function(e) {
      warning(paste("Error in reprojection:", e))
      return(NULL)
    })
    
    if (is.null(b_layer)) next
    
    # crop it to the boundary of interest
    b_layer_crop <- tryCatch({
      terra::crop(b_layer, terra::ext(boundary_load_rasters))
    }, error = function(e) {
      warning(paste("Error in cropping raster:", e))
      return(NULL)
    })
    
    if (is.null(b_layer_crop)) next
    
    # depending on what the band is, add it to the appropriate stack
    if (search_df$band[b] == 'B04') {
      l <- l + 1
      red_stack[[l]] <- b_layer_crop
      
      # only save dates for one if statement assuming that
      # all the bands have data at the same time
      # later use this to name the layers
      date_vect[l] <- ifelse(lab == 'S', paste0('S', as.character(dat)), paste0('L', as.character(dat)))
      
    } else if (search_df$band[b] == 'Fmask') {
      m <- m + 1
      fmask_stack[[m]] <- b_layer_crop
      
    } else if (search_df$band[b] == 'B05' || search_df$band[b] == 'B8A') {
      n <- n + 1
      nir_stack[[n]] <- b_layer_crop
      
    } else if (search_df$band[b] == 'B12' || search_df$band[b] == 'B07') {
      k <- k + 1
      swir2_stack[[k]] <- b_layer_crop
    }
  }
  # close the progress bar
  close(pb)
  
  # return all the stacks
  return(list(red_stack, nir_stack, fmask_stack, swir2_stack, date_vect))
}

```

Create functions to calculate NDVI and NBR.

```{r}
calculate_NDVI <- function(nir, red){
  valid_mask <- (nir + red) != 0
  ndvi <- (nir[valid_mask] - red[valid_mask]) / (nir[valid_mask] + red[valid_mask])
  result <- rep(NA, length(nir)) # Initialize with NA so that's the default if there aren't results
  result[valid_mask] <- ndvi
  return(result)
}

calculate_NBR <- function(nir, swir2){
  valid_mask <- (nir + swir2) != 0
  nbr <- (nir[valid_mask] - swir2[valid_mask]) / (nir[valid_mask] + swir2[valid_mask])
  result <- rep(NA, length(nir)) 
  result[valid_mask] <- nbr
  return(result)
}
```

Make a function to calculate NDVI and NBR for one scene, given inputs of stacks of rasters containing layers with all the data for the scene.

```{r, warning=FALSE}
make_one_scene <- function(red_stack, nir_stack, fmask_stack, swir2_stack, date_var){
  # input stacks for just one scene
  ndvi_stack <- nbr_stack <- list()
  
  for (i in 1:length(nir_stack)){
    # calculate NDVI
    ndvi <- calculate_NDVI(nir_stack[[i]], red_stack[[i]])
    # exclude non-finite values
    ndvi[!is.finite(ndvi)] <- NA
    ndvi_stack[[i]] <- ndvi
    names(ndvi_stack[[i]]) <- date_var
  
    # calculate NBR
    nbr <- calculate_NBR(nir_stack[[i]], swir2_stack[[i]])
    # exclude non-finite values
    nbr[!is.finite(nbr)] <- NA
    nbr_stack[[i]] <- nbr
    names(nbr_stack[[i]]) <- date_var
  }
  
  # tile together the NDVI, NBR, and Fmask scenes
  ndvi_scene <- mosaic(sprc(ndvi_stack))
  nbr_scene <- mosaic(sprc(nbr_stack))
  fmask_scene <- mosaic(sprc(fmask_stack))
  names(ndvi_scene) <- names(nbr_scene) <- names(fmask_scene) <- date_var
  
  return(list(ndvi_scene, nbr_scene, fmask_scene))
}

```

Create a filter function using fmask. For fmask, the 7-6th bits must be 00, indicating climatology-level aerosol levels, or 01, indicating low aerosol levels. The fmask value is 255 or below, so it's sufficient to check if the values is less than 128, because if so, the 7th bit is zero. The 6th bit can be either 0 or 1, so no further checking should be needed, I think...

```{r, warning=FALSE}
filter_rasters <- function(ndvi_scene,nbr_scene,fmask_scene) 
 {
  mask_raster <- fmask_scene
  # if it sucks, hit da bricks
  mask_raster[mask_raster >= 128] <- NA
  ndvi_filtered <- mask(ndvi_scene, mask_raster, maskvalue=NA )
  nbr_filtered <- mask(nbr_scene, mask_raster, maskvalue=NA )
  names(ndvi_filtered) <- names(ndvi_scene)
  names(nbr_filtered) <- names(nbr_scene)
  
  return(list(ndvi_filtered,nbr_filtered))
  }

```

Create a function to export a date's NDVI, NBR, and Fmask.

```{r, warning = FALSE}
export_one_layer <- function(ndvi_scene,nbr_scene,fmask_scene, outdir){
  output_name_ndvi <- file.path(outdir, paste0(names(ndvi_scene), "_NDVI.tif"))
  output_name_nbr <- file.path(outdir, paste0(names(nbr_scene), "_NBR.tif"))
  output_name_fmask <- file.path(outdir, paste0(names(nbr_scene), "_fmask.tif"))
  terra::writeRaster(ndvi_scene, output_name_ndvi, overwrite = TRUE)
  terra::writeRaster(nbr_scene, output_name_nbr, overwrite = TRUE)
  terra::writeRaster(fmask_scene, output_name_fmask, overwrite = TRUE)
}
```


Main: call the functions needed to search for, load, and process the HLS data to save each layer individually without filtering.
```{r, warning=FALSE}
# assign search url
search_URL <- 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/search'

# assign data names
HLS_col <- list("HLSS30.v2.0", "HLSL30.v2.0")

# retrieve spatial boundary
boundary_vect <- terra::vect("Data/CreekFire_1km_buffer.geojson")

# get spatial extent of boundary
roi <- terra::ext(boundary_vect) # ext gets a spatial extent
roi_bbox <- paste(roi[1], roi[3], roi[2], roi[4], sep = ',')
prefire_et <- terra::rast("Data/openET_data/tiffs/prefire_mean_ET.tif")

# set up date ranges of interest

# Prefire 2020-06-01 to 2020-08-31
# Postfire 2021-06-01 to 2020-08-31
date_ranges <- c('2020-06-01T00:00:00Z/2020-07-01T00:00:00Z',
                 '2020-07-01T00:00:00Z/2020-08-01T00:00:00Z',
                 '2020-08-01T00:00:00Z/2020-09-01T00:00:00Z',
                 '2021-06-01T00:00:00Z/2021-07-01T00:00:00Z',
                 '2021-07-01T00:00:00Z/2021-08-01T00:00:00Z',
                 '2021-08-01T00:00:00Z/2021-09-01T00:00:00Z')

# test a small subset:
#date_ranges <- c('2020-06-01T00:00:00Z/2020-06-10T23:59:59Z')

for (i in 1:length(date_ranges)) {
  print(date_ranges[i])
  creekfire_search_req <- get_search(date_ranges[i],roi_bbox) # roi_bbox defined above
  
  # this takes in an extent string
  search_df <- make_table(creekfire_search_req$features,i)
  
  # only use features without excess cloud cover
  search_df <- search_df[search_df$Cloud_Cover < 30, ]
  
  raster_output <-  load_rasters(search_df,boundary_vect)
  red_stack <- raster_output[[1]]
  nir_stack <- raster_output[[2]]
  fmask_stack <- raster_output[[3]]
  swir2_stack <- raster_output[[4]]
  date_vect <- raster_output[[5]]
  cat("Rasters loaded for", date_ranges[i], "\n")
  
  bands <- c("red","nir","swir2","fmask")
  stacks <- list(red_stack,nir_stack,swir2_stack,fmask_stack)


  # merge them and save rasters covering the whole area and the individuals too
  for (k in 1:length(bands)) {
    # for each unique scene date,
    for (d in 1:length(unique(date_vect))) {
      # get the date
      scene_date <- unique(date_vect)[d]
      
      # find the indices of the rasters for the scene
      scene_indices <- which(date_vect == scene_date)
      
      # subset the stack data for that scene
      scene_data <- stacks[[k]][scene_indices] 
      # kth list item is a list and [c(...)] should return spatraster items from it
      # so scene_data is a list of rasters in one scene for one band
      
      # need to resample to match resolution for every raster in the scene
      # so I can mosaic the scene together
      # this methodology seems somewhat questionable, ask someone about it
      scene_data_resampled <- lapply(scene_data,FUN=function(x){terra::resample(x,prefire_et,method="near")}) # match res to et
      
      # save these all to be safe; label the pieces of the scene 1,...,h,
      for (h in 1:length(scene_data_resampled)) {
        terra::writeRaster(scene_data_resampled[[h]],paste0(outDir,"/",bands[k],"/",bands[k],"_",
                                                    scene_date,"_",h,".tiff"),
                         overwrite=TRUE)
      }
      
      # mosaic them together
      scene_data_merged <- terra::mosaic(terra::sprc(scene_data_resampled))
      
      # save mosaicked file
      terra::writeRaster(scene_data_merged,paste0(outDir,"/",bands[k],"/",bands[k],"_",
                                                    scene_date,"_mosaicked.tiff"),
                         overwrite=TRUE)
    }
  }
}



```
