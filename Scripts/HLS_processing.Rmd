---
title: "HLS Aquisition and Processing"
output:
  html_document:
    df_print: paged
    fig_caption: yes
    theme: paper
    toc: yes
    toc_depth: 2
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '2'
  word_document:
    toc: yes
    toc_depth: '2'
theme: lumen
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(comment = NA)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
```

Adapted from HLS R tutorial: https://git.earthdata.nasa.gov/scm/lpdur/hls_tutorial_r.git

Call .netrc file

```{r, warning = FALSE, message = FALSE, results= "hide"}
source("Scripts/earthdata_netrc_setup.R")
```

Check required packages and install as needed.

```{r, warning = FALSE, message = FALSE}
packages <- c('terra','jsonlite','sp','httr',
              'rasterVis','ggplot2','magrittr','RColorBrewer','xml2','dygraphs',
              'xts','lubridate','DT','rmarkdown', 'rprojroot','imager','htmltools',
              'tidyterra','leaflet')

# Identify missing (not installed) packages
new.packages = packages[!(packages %in% installed.packages()[,"Package"])]

# Install new (not installed) packages
if(length(new.packages)) install.packages(new.packages, repos='http://cran.rstudio.com/') else print('All required packages are installed.')
```

Load packages.

```{r, warning= FALSE, message=FALSE}
invisible(lapply(packages, library, character.only = TRUE))
```

Create output directory if needed.

```{r}
# Create an output directory if it doesn't exist
wd <- rprojroot::find_rstudio_root_file()
outDir <- file.path(wd, "Data", "HLS_data", fsep="/")
suppressWarnings(dir.create(outDir)) 
```

Create search query function.

```{r}
get_search <- function(date_range,boundary_str) {
  # input date_range (vec of str) and boundary_str (str)
  search_body <- list(limit=250,
                 datetime=date_range,
                 bbox= boundary_str,
                 collections= HLS_col)
  
  search_req <- httr::POST(search_URL, body = search_body, encode = "json") %>%
    httr::content(as = "text") %>%
    fromJSON()
  
  cat('There are',search_req$numberMatched, 'features that matched our request for', date_ranges[i], "\n")

  return(search_req)
}
```

Set gdal configurations (I don't know what this stuff is).

```{r, results= "hide"}
setGDALconfig("GDAL_HTTP_UNSAFESSL", value = "YES")
setGDALconfig("GDAL_HTTP_COOKIEFILE", value = ".rcookies")
setGDALconfig("GDAL_HTTP_COOKIEJAR", value = ".rcookies")
setGDALconfig("GDAL_DISABLE_READDIR_ON_OPEN", value = "EMPTY_DIR")
setGDALconfig("CPL_VSIL_CURL_ALLOWED_EXTENSIONS", value = "TIF")
```

Bands of interest:

-   Sentinel-2:

    -   "narrow" NIR = B8A
    -   Red = B04
    -   SWIR 1 = B11
    -   SWIR 2 = B12
    -   Quality = Fmask

-   Landsat 8:

    -   NIR = B05
    -   Red = B04
    -   SWIR 1 = B06
    -   SWIR 2 = B07
    -   Quality = Fmask

Create a function to make a searchable datatable from which to retrieve asset urls.
- I don't have a great understanding of this function that came from the tutorial

```{r}
make_table <- function(search_features, i) {

  granule_list <- list()
  n <- 1
  for (item in row.names(search_features)){                       # Get the NIR, Red, SWIR2 and Quality band layer names
    if (search_features[item,]$collection == 'HLSS30.v2.0'){
      ndvi_bands <- c('B8A','B04','B12','Fmask')
    }
    else{
      ndvi_bands <- c('B05','B04','B07','Fmask')
    }
    for(b in ndvi_bands){
      f <- search_features[item,]
      b_assets <- f$assets[[b]]$href
      
      df <- data.frame(Collection = f$collection,                    # Make a data frame including links and other info
                       Granule_ID = f$id,
                       Cloud_Cover = f$properties$`eo:cloud_cover`,
                       band = b,
                       Asset_Link = b_assets, stringsAsFactors=FALSE)
      granule_list[[n]] <- df
      n <- n + 1
    }
  }
  
  # Create a searchable datatable
  search_df <- do.call(rbind, granule_list)
  return(search_df)
}
```

Check .netrc location.

```{r, echo = FALSE, results='hide'}
cat('The netrc file can be found in:', Sys.getenv('HOME'))
```

Create a function to switch the boundary's crs to the crs found in the data.

```{r, warning=FALSE}
update_crs <- function(search_df,boundary_vec) {
  # extract data's crs
  coordinate_reference <- terra::rast(paste0(search_df$Asset_Link[1]))
  
  # update boundary's crs
  boundary_updated <- terra::project(boundary_vec, crs(coordinate_reference)) # Transfer CRS
  # crs(creekfire_geojson_utm)
  return(boundary_updated)
}
```

Create a function to load the rasters from the datatable retrieved.

```{r, warning=FALSE, results= "hide"}
# load_rasters <- function(search_df) {
#   
#   # initialize stacks
#   red_stack <- nir_stack <- fmask_stack <- swir2_stack <- list()
#   date_vec <- c()
#   
#   # create progress bar
#   pb <- txtProgressBar(min = 0, max = length(search_df$band), initial = 0, style = 3)
#   
#   # initialize counters for each stack
#   l <- m <- n <- k <- 0
#   
#   # create function to extract date from raster source
#   extract_date <- function(granule_id) {
#     doy_time = strsplit(granule_id, "[.]")[[1]][4]
#     doy = substr(doy_time, 1, 7)
#     dat <- as.Date(as.integer(substr(doy,5,7)), origin = paste0(substr(doy,1,4), "-01-01"))
#     return(dat)
#   }
#   
#   # go through every row in the search_df
#   for (b in 1:length(search_df$band)) {
#     
#       # increment the progress bar
#       setTxtProgressBar(pb, b)
#     
#       # retrieve this file
#       b_layer <- terra::rast(paste0('/vsicurl/', search_df$Asset_Link[b])) 
#       lab <- substr(search_df$Granule_ID[b],5,5)
#       dat <- extract_date(search_df$Granule_ID[b])
#       
#       # project it to WGS
#       b_layer <- terra::project(b_layer, "EPSG:4326")
#       
#       # crop it to the boundary of interest
#       b_layer_crop <- terra::crop(b_layer, terra::ext(boundary_load_rasters))
#     
#       # depending on what the band is, add it to the appropriate stack
#       if (search_df$band[b] == 'B04') {
#         l = l + 1
#         red_stack[[l]] <- b_layer_crop
#         
#         # only save dates for one if statement assuming that
#         # all the bands have data at the same time
#         date_vec[l] <- ifelse(lab == 'S', paste0('S', as.character(dat)), paste0('L', as.character(dat)))
#         
#       } else if (search_df$band[b] == 'Fmask') {
#         m = m + 1
#         fmask_stack[[m]] <- b_layer_crop
#         
#       } else if (search_df$band[b] == 'B05' || search_df$band[b] == 'B8A') {
#         n = n + 1
#         nir_stack[[n]] <- b_layer_crop
#         
#       } else if (search_df$band[b] == 'B12' || search_df$band[b] == 'B07') {
#         k = k + 1
#         swir2_stack[[k]] <- b_layer_crop
#       }
# 
#   }
#   # close the progress bar
#   close(pb)
#   
#   # return all the stacks
#   return(list(red_stack, nir_stack, fmask_stack, swir2_stack, date_vec))
# }


load_rasters <- function(search_df, boundary_load_rasters) {
  
  # initialize stacks
  red_stack <- nir_stack <- fmask_stack <- swir2_stack <- list()
  date_vec <- c()
  
  # create progress bar
  pb <- txtProgressBar(min = 0, max = length(search_df$band), initial = 0, style = 3)
  
  # initialize counters for each stack
  l <- m <- n <- k <- 0
  
  # create function to extract date from raster source
  extract_date <- function(granule_id) {
    doy_time <- strsplit(granule_id, "[.]")[[1]][4]
    doy <- substr(doy_time, 1, 7)
    dat <- as.Date(as.integer(substr(doy, 5, 7)), origin = paste0(substr(doy, 1, 4), "-01-01"))
    return(dat)
  }
  
  # go through every row in the search_df
  for (b in 1:length(search_df$band)) {
    
    # increment the progress bar
    setTxtProgressBar(pb, b)
    
    # retrieve this file
    b_layer <- tryCatch({
      terra::rast(paste0('/vsicurl/', search_df$Asset_Link[b]))
    }, error = function(e) {
      warning(paste("Error loading raster:", e))
      return(NULL)
    })
    
    if (is.null(b_layer)) next
    
    lab <- substr(search_df$Granule_ID[b], 5, 5)
    dat <- extract_date(search_df$Granule_ID[b])
    
    # project it to WGS
    b_layer <- tryCatch({
      terra::project(b_layer, "EPSG:4326")
    }, error = function(e) {
      warning(paste("Error in reprojection:", e))
      return(NULL)
    })
    
    if (is.null(b_layer)) next
    
    # crop it to the boundary of interest
    b_layer_crop <- tryCatch({
      terra::crop(b_layer, terra::ext(boundary_load_rasters))
    }, error = function(e) {
      warning(paste("Error in cropping raster:", e))
      return(NULL)
    })
    
    if (is.null(b_layer_crop)) next
    
    # depending on what the band is, add it to the appropriate stack
    if (search_df$band[b] == 'B04') {
      l <- l + 1
      red_stack[[l]] <- b_layer_crop
      
      # only save dates for one if statement assuming that
      # all the bands have data at the same time
      date_vec[l] <- ifelse(lab == 'S', paste0('S', as.character(dat)), paste0('L', as.character(dat)))
      
    } else if (search_df$band[b] == 'Fmask') {
      m <- m + 1
      fmask_stack[[m]] <- b_layer_crop
      
    } else if (search_df$band[b] == 'B05' || search_df$band[b] == 'B8A') {
      n <- n + 1
      nir_stack[[n]] <- b_layer_crop
      
    } else if (search_df$band[b] == 'B12' || search_df$band[b] == 'B07') {
      k <- k + 1
      swir2_stack[[k]] <- b_layer_crop
    }
  }
  # close the progress bar
  close(pb)
  
  # return all the stacks
  return(list(red_stack, nir_stack, fmask_stack, swir2_stack, date_vec))
}

```

Create functions to calculate NDVI and NBR.

```{r}
calculate_NDVI <- function(nir, red){
  valid_mask <- (nir + red) != 0
  ndvi <- (nir[valid_mask] - red[valid_mask]) / (nir[valid_mask] + red[valid_mask])
  result <- rep(NA, length(nir)) # Initialize with NA so that's the default if there aren't results
  result[valid_mask] <- ndvi
  return(result)
}

calculate_NBR <- function(nir, swir2){
  valid_mask <- (nir + swir2) != 0
  nbr <- (nir[valid_mask] - swir2[valid_mask]) / (nir[valid_mask] + swir2[valid_mask])
  result <- rep(NA, length(nir)) 
  result[valid_mask] <- nbr
  return(result)
}
```

Make a function to calculate NDVI and NBR for one scene, given inputs of stacks of rasters containing layers with all the data for the scene.

```{r, warning=FALSE}
make_one_scene <- function(red_stack, nir_stack, fmask_stack, swir2_stack, date_var){
  # input stacks for just one scene
  ndvi_stack <- nbr_stack <- list()
  
  for (i in 1:length(nir_stack)){
    # Calculate NDVI
    ndvi <- calculate_NDVI(nir_stack[[i]], red_stack[[i]])
    # Exclude non-finite values
    ndvi[!is.finite(ndvi)] <- NA
    ndvi_stack[[i]] <- ndvi
    names(ndvi_stack[[i]]) <- date_var
  
    # Calculate NBR
    nbr <- calculate_NBR(nir_stack[[i]], swir2_stack[[i]])
    # Exclude non-finite values
    nbr[!is.finite(nbr)] <- NA
    nbr_stack[[i]] <- nbr
    names(nbr_stack[[i]]) <- date_var
  }
  
  # Tile together the ndvi, nbr, and fmask scenes
  ndvi_scene <- mosaic(sprc(ndvi_stack))
  nbr_scene <- mosaic(sprc(nbr_stack))
  fmask_scene <- mosaic(sprc(fmask_stack))
  names(ndvi_scene) <- names(nbr_scene) <- names(fmask_scene) <- date_var
  
  return(list(ndvi_scene, nbr_scene, fmask_scene))
}

```

Plotting:
```{r}
# # Get global max and min
# color_scale <- c(0,1) #c(min(terra::global(ndvi_scene_stack,fun="min",na.rm=TRUE)$min),max(terra::global(ndvi_scene_stack,fun="max",na.rm=TRUE)$max))
# 
# # Make color palette
# pal <- colorNumeric(terrain.colors(n = 100),color_scale,na.color = "transparent", reverse = TRUE)
# 
# # Plot ET
# leaflet() %>% 
#     addProviderTiles(providers$Esri.WorldImagery) %>%
#     addRasterImage(ndvi_scene_stack[[1]], color = pal, opacity = 1) %>%
#     addPolygons(data = creekfire_geojson, fill = FALSE) %>%
#     addMiniMap(zoomLevelFixed = 10) %>%
#     leaflet::addLegend(pal = pal, values = color_scale, title = "ET")

```

NBR plots

```{r, warning=FALSE, message=FALSE}
# Create a color palette 
# pal <- colorNumeric(terrain.colors(n = 100), c(0,1) ,na.color = "transparent", reverse = TRUE)
# 
# 
# # Plot NBR
# leaflet() %>% 
#     addProviderTiles(providers$Esri.WorldImagery) %>%
#     addRasterImage(nbr_scene_stack[[1]], color = pal, opacity = 1) %>%
#     addPolygons(data = creekfire_geojson, fill = FALSE) %>%
#     addMiniMap(zoomLevelFixed = 5) %>%
#     leaflet::addLegend(pal = pal, values = c(0,1), title = "NBR")


```


Create a filter function using fmask. For fmask, the 7-6th bits must be 00, indicating climatology-level aerosol levels, or 01, indicating low aerosol levels. The fmask value is 255 or below, so it's sufficient to check if the values is less than 128, because if so, the 7th bit is zero. The 6th bit can be either 0 or 1, so no further checking should be needed.

```{r, warning=FALSE}
filter_rasters <- function(ndvi_scene,nbr_scene,fmask_scene) 
 {
  mask_raster <- fmask_scene
  mask_raster[mask_raster >= 128] <- NA
  ndvi_filtered <- mask(ndvi_scene, mask_raster, maskvalue=NA )
  nbr_filtered <- mask(nbr_scene, mask_raster, maskvalue=NA )
  names(ndvi_filtered) <- names(ndvi_scene)
  names(nbr_filtered) <- names(nbr_scene)
  
  return(list(ndvi_filtered,nbr_filtered))
  }

```

Visualization:

```{r, warning=FALSE, message=FALSE}
# 
# base<-c('map<-leaflet()%>%
#             addProviderTiles(providers$Esri.WorldImagery) %>%
#             addMiniMap(zoomLevelFixed = 5) %>%')
# 
# # make a string including the addRasterImage function for every layer in the raster stack
# 
# X <- lapply(1:nlyr(ndvi_scene_stacked), function(j){
#     paste(paste("addRasterImage(ndvi_scene_stacked[[",j,"]],
#              colors = pal,
#              opacity=1,
#              group=names(ndvi_scene_stacked[[",j,"]]))", sep=""),"%>% \n")})
# 
# X <- do.call(paste, X)
# 
# controls<-"addLayersControl(baseGroups=names(ndvi_scene_stacked),
#                options = layersControlOptions(collapsed=F), position = 'topleft')%>%"
# 
# legend <- "leaflet::addLegend(pal = pal, values = c(0,1), title = 'NDVI')"
# 
# final <- paste(base, X, controls, legend ,sep="\n")
# eval(parse(text=final))
# map
```

```{r, warning=FALSE, message=FALSE}
# 
# base<-c('map<-leaflet()%>%
#             addProviderTiles(providers$Esri.WorldImagery) %>%
#             addMiniMap(zoomLevelFixed = 5) %>%')
# 
# # make a string including the addRasterImage function for every layer in the raster stack
# 
# X <- lapply(1:nlyr(ndvi_filtered_stack), function(j){
#     paste(paste("addRasterImage(ndvi_filtered_stack[[",j,"]],
#              colors = pal,
#              opacity=1,
#              group=names(ndvi_filtered_stack[[",j,"]]))", sep=""),"%>% \n")})
# 
# X <- do.call(paste, X)
# 
# controls<-"addLayersControl(baseGroups=names(ndvi_filtered_stack),
#                options = layersControlOptions(collapsed=F), position = 'topleft')%>%"
# 
# legend <- "leaflet::addLegend(pal = pal, values = c(0,1), title = 'NDVI')"
# 
# final <- paste(base, X, controls, legend ,sep="\n")
# eval(parse(text=final))
# map
```

Create a function to export a date's NDVI, NBR, and Fmask.

```{r, warning = FALSE}
export_one_layer <- function(ndvi_scene,nbr_scene,fmask_scene, outdir){
  output_name_ndvi <- file.path(outdir, paste0(names(ndvi_scene), "_NDVI.tif"))
  output_name_nbr <- file.path(outdir, paste0(names(nbr_scene), "_NBR.tif"))
  output_name_fmask <- file.path(outdir, paste0(names(nbr_scene), "_fmask.tif"))
  terra::writeRaster(ndvi_scene, output_name_ndvi, overwrite = TRUE)
  terra::writeRaster(nbr_scene, output_name_nbr, overwrite = TRUE)
  terra::writeRaster(fmask_scene, output_name_fmask, overwrite = TRUE)
}
```

Create a function to do the same for stacks of data over a date range.

```{r, warning=FALSE}
export_stacks <- function(ndvi_filtered_stack,nbr_filtered_stack, i) {
  output_name_ndvi_stack <- paste0(outDir,"/NDVI_stack_",substr(date_ranges[i],start=1,stop=10),"_",substr(date_ranges[i],start=22,stop=31),".tif")
  output_name_nbr_stack <- paste0(outDir,"/NDVI_stack_",substr(date_ranges[i],start=1,stop=10),"_",substr(date_ranges[i],start=22,stop=31),".tif")
  terra::writeRaster(ndvi_filtered_stack,filename=output_name_ndvi_stack, overwrite=TRUE)
  terra::writeRaster(nbr_filtered_stack,filename=output_name_nbr_stack, overwrite=TRUE)
}
```


Call the functions needed to search for, load, and process the HLS data to save each layer individualy without filtering.
```{r, warning=FALSE}
# Assign search url
search_URL <- 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/search'

# Assign data names
HLS_col <- list("HLSS30.v2.0", "HLSL30.v2.0")

# Retrieve spatial boundary
boundary_vec <- terra::vect("Data/CreekFire_1km_buffer.geojson")

# Get spatial extent of boundary
roi <- terra::ext(boundary_vec) # ext gets a spatial extent
roi_bbox <- paste(roi[1], roi[3], roi[2], roi[4], sep = ',')
prefire_et <- terra::rast("Data/openET_data/tiffs/prefire_mean_ET.tif")

# Set up date ranges of interest

# Prefire 2020-06-01 to 2020-08-31
# Postfire 2021-06-01 to 2020-08-31
date_ranges <- c('2020-06-01T00:00:00Z/2020-07-01T00:00:00Z',
                 '2020-07-01T00:00:00Z/2020-08-01T00:00:00Z',
                 '2020-08-01T00:00:00Z/2020-09-01T00:00:00Z',
                 '2021-06-01T00:00:00Z/2021-07-01T00:00:00Z',
                 '2021-07-01T00:00:00Z/2021-08-01T00:00:00Z',
                 '2021-08-01T00:00:00Z/2021-09-01T00:00:00Z')

# test a small subset:
#date_ranges <- c('2020-06-01T00:00:00Z/2020-06-10T23:59:59Z')

for (i in 1:1) {
  print(date_ranges[i])
  creekfire_search_req <- get_search(date_ranges[i],roi_bbox)
  # this takes in an extent string
  search_df <- make_table(creekfire_search_req$features,i)
  
  # Only use features without excess cloud cover
  search_df <- search_df[search_df$Cloud_Cover < 30, ]
  
  boundary_vec_utm <- update_crs(search_df,boundary_vec)
  roi_utm <- terra::ext(boundary_vec_utm)
  roi_bbox_utm <- paste(roi_utm[1], roi_utm[3], roi_utm[2], roi_utm[4], sep = ',') # check that this is right
  
  raster_output <-  load_rasters(search_df,boundary_vec)
  red_stack <- raster_output[[1]]
  nir_stack <- raster_output[[2]]
  fmask_stack <- raster_output[[3]]
  swir2_stack <- raster_output[[4]]
  date_vec <- raster_output[[5]]
  cat("Rasters loaded for", date_ranges[i], "\n")
  
  bands <- c("red","nir","swir2","fmask")
  stacks <- list(red_stack,nir_stack,swir2_stack,fmask_stack)


  # merge them and save rasters covering the whole area and the individuals too
  for (k in 1:length(bands)) {
    # for each unique scene date,
    for (d in 1:length(unique(date_vec))) {
      # get the date
      scene_date <- unique(date_vec)[d]
      
      # find the indices of the rasters for the scene
      scene_indices <- which(date_vec == scene_date)
      
      # subset the stack data for that scene
      scene_data <- stacks[[k]][scene_indices] 
      # kth list item is a list and [c(...)] should return spatraster items from it
      # so scene_data is a list of rasters in one scene for one band
      
      # need to resample to match resolution for every raster in the scene
      # just resample from first raster in the stack... pray this works
      # I'd expect that to be helpful because then all scenes for
      # one band should have the same resolution...
      scene_data_resampled <- lapply(scene_data,FUN=function(x){terra::resample(x,prefire_et)}) # match res to et
      
      # save these all to be safe; label the pieces of the scene 1,...,h,
      for (h in 1:length(scene_data_resampled)) {
        terra::writeRaster(scene_data_resampled[[h]],paste0(outDir,"/",bands[k],"/",bands[k],"_",
                                                    scene_date,"_",h,".tiff"),
                         overwrite=TRUE)
      }
      
      # Finally, merge them together
      # I think it would be better to mosaic, but time is a concern
      scene_data_merged <- terra::merge(terra::sprc(scene_data_resampled))
      
      # Save this file
      terra::writeRaster(scene_data_merged,paste0(outDir,"/",bands[k],"/",bands[k],"_",
                                                    scene_date,"_merged.tiff"),
                         overwrite=TRUE)
    }
  }
}


# Check out one of the scenes ...
test_scene <- terra::rast(paste0(outDir,"/fmask/fmask_L2020-07-27_merged.tiff"))


check_url <- terra::rast("https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/HLSS30.020/HLS.S30.T11SLB.2020197T183919.v2.0/HLS.S30.T11SLB.2020197T183919.v2.0.B8A.tif")
ggplot() +
  geom_spatraster(data=test_scene)
  
```


```{r}
# search_URL <- 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD/search'
# HLS_col <- list("HLSS30.v2.0", "HLSL30.v2.0")
# boundary_vec <- terra::vect("Data/CreekFire_1km_buffer.geojson")
# roi <- terra::ext(boundary_vec)
# roi_bbox <- paste(roi[1], roi[3], roi[2], roi[4], sep = ',')
# prefire_et <- terra::rast("Data/openET_data/tiffs/prefire_mean_ET.tif")
# date_ranges <- c('2020-06-01T00:00:00Z/2020-07-01T00:00:00Z',
#                  '2020-07-01T00:00:00Z/2020-08-01T00:00:00Z',
#                  '2020-08-01T00:00:00Z/2020-09-01T00:00:00Z',
#                  '2021-06-01T00:00:00Z/2021-07-01T00:00:00Z',
#                  '2021-07-01T00:00:00Z/2021-08-01T00:00:00Z',
#                  '2021-08-01T00:00:00Z/2021-09-01T00:00:00Z')
# 
# # test a small subset:
# # date_ranges <- c('2020-06-01T00:00:00Z/2020-06-10T23:59:59Z')
# 
# for (i in 1:1) {
#   print(date_ranges[i])
#   creekfire_search_req <- get_search(date_ranges[i], roi_bbox)
#   search_df <- make_table(creekfire_search_req$features, i)
#   
#   # Only use features without excess cloud cover
#   search_df <- search_df[search_df$Cloud_Cover < 30, ]
#   
#   boundary_vec_utm <- update_crs(search_df, boundary_vec)
#   roi_utm <- terra::ext(boundary_vec_utm)
#   roi_bbox_utm <- paste(roi_utm[1], roi_utm[3], roi_utm[2], roi_utm[4], sep = ',')
#   
#   raster_output <- load_rasters(search_df, boundary_vec)
#   red_stack <- raster_output[[1]]
#   nir_stack <- raster_output[[2]]
#   fmask_stack <- raster_output[[3]]
#   swir2_stack <- raster_output[[4]]
#   date_vec <- raster_output[[5]]
#   cat("Rasters loaded for", date_ranges[i], "\n")
#   
#   bands <- c("red", "nir", "swir2", "fmask")
#   stacks <- list(red_stack, nir_stack, swir2_stack, fmask_stack)
#   
#   for (k in 1:length(bands)) {
#     for (j in 1:length(unique(date_vec))) {
#       scene_date <- unique(date_vec)[j]
#       scene_indices <- which(date_vec == scene_date)
#       scene_data <- stacks[[k]][scene_indices]
#       
#       # Check if scene_data is not empty
#       if (length(scene_data) == 0) {
#         next
#       }
#       
#       scene_data_resampled <- lapply(scene_data, FUN = function(x) {
#         terra::resample(x, prefire_et)
#       })
#       
#       for (h in 1:length(scene_data_resampled)) {
#         terra::writeRaster(scene_data_resampled[[h]], paste0(outDir, "/", bands[k], "/", bands[k], "_", scene_date, "_", h, ".tiff"), overwrite = TRUE)
#       }
#       
#       scene_data_merged <- terra::merge(terra::sprc(scene_data_resampled))
#       terra::writeRaster(scene_data_merged, paste0(outDir, "/", bands[k], "/", bands[k], "_", scene_date, "_merged.tiff"), overwrite = TRUE)
#     }
#   }
# }
```






